KNN classification and feature set selection 
============================================

2014 07 02 

We use scikit to perform kNN classification.

Knn with default params, 15 neighbors.

##Â With a training ratio of 0.8, 

On the 3-mers, we manage a kappa of 0.66 with 15 neighbors. Classification time is approx 7 seconds. 

On the 5-mers, we manage a kappa of 0.64. Normalization does increase the perf


We randomly subset  kmers and perform the classification on the same dataset. 

On 8 randomly selected k-mers, we get a kappa after 1.53 s

We compute for random selection of k-mer subset, ranging from 8 to all 64 3-mers (cf [knn_feature_subsets.py]). The best k-mer subsets we get are : 
|    32   | 1010111001000000001111011001010010011001101000101111101001011011 | 0.606060945467 |
|    32   | 0011111110111101011011111101100100001001000001100000010011000011 | 0.606144837913 |
|    32   | 1010000000010010011011111101000011110011010000111101100010011111 | 0.606606499261 |
|    32   | 1111100000111110111010100001111100101001000100010011000011001011 | 0.606717187496 |
|    32   | 0001101000101011011111001101100100110111010000010110101100110100 | 0.607782721646 |
|    32   | 1101100110101100001100000101010001101111011100001010101100110011 | 0.608069274502 |
|    32   | 0010111010100111000010011001010101101000110011111101011011100000 | 0.608137912331 |
|    32   | 1001101001100110010001010110001101101000100011011111010010001111 |  0.6083174679  |
|    32   | 1000011111100000000000101100011001011101110001111110101001101011 | 0.608452085398 |
|    32   | 1100111011111001111000001001101000101011001000010000011110110101 | 0.609100411432 |
|    32   | 0000111100010100011101111000101110111110000111101000011000001101 | 0.609356142791 |
|    32   | 1001111010011000001101011010101001111011100001011010110000001101 | 0.610031879942 |
|    32   | 0011110001001111100111000010110001010001110011001110001001100111 | 0.610677881033 |
|    32   | 1101100001110101100010010110011011011110000001110001010110010110 | 0.611253606078 |
|    32   | 1101100110011101100010100101010101100011101010010001001001110110 | 0.611648966304 |
|    32   | 1000000010100010111100111011001111000011000111101110010001011101 | 0.611863265194 |
|    32   | 0100110111111111101001100000110110011001101101001000001010010100 | 0.612675864528 |
|    32   | 1111100001001011101001010101001001101010010110110010101110001010 | 0.613497494711 |
|    32   | 1100011001101001001111000100110001011001101010101101011000010111 | 0.614509719249 |
|    32   | 0011100000111110001001001000111001001111100001101100110101101011 | 0.615265140126 |
|    32   | 0000111111001011010011110110000100100000100001100111101101101011 | 0.61708477483  |
|    32   | 0010110110010100001011100000110111010101100100101011011011001011 | 0.618052220746 |
|    52   | 1011101110111101111111111010111111111110010001111011111111111111 | 0.65591889339  |
|    52   | 1111100110111111101010101111010111111111111011101111111111011111 | 0.656031827244 |
|    54   | 1111110111111101010101111111111111111111101011111011111110111011 | 0.656333658593 |
|    54   | 1111101101111101110110011101111111110111111111111101011111111111 | 0.656588182772 |
|    54   | 1111111111011011011111011101111011111101111111111111111100111011 | 0.656652728895 |
|    54   | 1111101111101101101111111111011011111111111101111111101111011011 | 0.656734918824 |
|    54   | 1110101011100101111111111011111111111111011111111011111101111111 | 0.65746250063  |
|    54   | 0111111111111101111111111111101111101110001110001111111111111111 | 0.657739138456 |
|    54   | 0111111011110111111111111001011101111111011110111111111111111011 | 0.657821374177 |
|    54   | 1111100111111011111010111101111111011111101011111111011111111111 | 0.657944067389 |
|    54   | 1011111111111111111101111101111111110011101111011111111101011110 | 0.657963309141 |
|    54   | 1111110111101111111111111111011111011111110111100011010111111111 | 0.658115945903 |
|    54   | 1110111111111111111001111111011111111011111111101011011110111011 | 0.658125116373 |
|    54   | 1111111111111111111001011111111111110011111011111110011011111110 | 0.658846034131 |
|    54   | 1111100111101111111111111101111111110111111111110111111011101010 | 0.659015036265 |
|    54   | 0111110111101111111011111011111111011111101011111111111110111110 | 0.661795632795 |
|    54   | 1011111111111111011111111011111111110110001111111111011100111111 | 0.662254881558 |
|    64   | 1111111111111111111111111111111111111111111111111111111111111111 | 0.667012317704 |
+---------+------------------------------------------------------------------+----------------+

definitely the best classifiers are obtained using 64 k-mers.




We do the same but with merging with 4 and 5 mers.

With 3...4-mers, the best is once again using all k-mers: 

|    60   | 00010000010000010100000011101000000010000101000001011000010000011100000000010010100000001000000010000010000000000000100000100000111000000100000101001000100001100011000000000100000000000000001000100000000000000000000000001001000000000000010000000100000100000000000001011000000000000000101000101000110101100000100000001000 | 0.578794097559 |
|    60   | 01101000000000000000000000000010000001100000000110101100000001000000110000000000000001000000110011000000101000000000100000000000000001000000100001000000000000000001001000001000000000000100001100000110000000100000010111100000000010000010000110000000000000000010000100010000010001011000101000001000110000000001010001100010 | 0.580221213542 |
|    60   | 00000000001100000010000010000001001001101000000001000010000000001010000111101011000101000000000010000000000100000000010010001100011000010000100000000000000100000000000010000000010000001000000010100001000000000101000000000000011010110100000000000000000010001010010010000000001101100000000000010000001001100000000000000000 | 0.580500363012 |
|    60   | 00001000000000000101000101010001001010000000000110100001000000101000000010000000001000000000000000010000010100100000011000010000000100010001101000000110000000011000000000100011000000000000001000000000000010000010000110000000000110100110000000000000000000000000000000100100001000001000001100100100011000000000100000101001 | 0.582696158431 |
|    60   | 00000000000001010000000001100000000000010000000011000010000000000010000011000000100001101001000010000001000000001100110001001000100110010001100001001000000000010001000010111001000000000000001100000110010000000000000001000000000001000000000000000010000000000000000000000000000000000011000011100100011010000010101000100000 | 0.585107380968 |
|    60   | 00100010100000000000000001100000001000000000000000100000100010000000101000000100100100010001100001111000001000100000000110000000001000000000101000000100001000000000110000101010000000000100000000000000000000010100010000011001000000000101101001000000100000000010000001000000110000001000000000000000000100100000001001101100 | 0.585960426256 |
|    64   | 10000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000100001000010000 | 0.664012942656 |
|   320   | 11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 | 0.666805892309 |
|   256   | 01111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111011110111101111 | 0.669737596629 |
+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+


If we start with all 64 k-mers, and just remove one k-mer, can we observe an improvement?



|    63   | 1111111111111111111111111111111111111111111101111111111111111111 | 0.663756801212 |
|    63   | 1111111111111111111111111111111111111111111111111111111101111111 | 0.663792385458 |
|    64   | 1111111111111111111111111111111111111111111111111111111111111111 | 0.664012942656 |
|    63   | 1111111111111111111111111111111111110111111111111111111111111111 | 0.664085601531 |
|    63   | 1111101111111111111111111111111111111111111111111111111111111111 | 0.66421763898  |
|    63   | 1111111111111111111111111111111111111111111111011111111111111111 | 0.664309399079 |
|    63   | 1111111111111111111111111111111111111111111111111111011111111111 | 0.664377058166 |
|    63   | 1111111111111111011111111111111111111111111111111111111111111111 | 0.664754734457 |
|    63   | 1111111111111111111111111101111111111111111111111111111111111111 | 0.664759742518 |
|    63   | 1111111111111111111111111111111011111111111111111111111111111111 | 0.664777537498 |
|    63   | 1111111111111111111111111111111111111111111111111111111011111111 | 0.664806259125 |
|    63   | 1111111111111111111111111111111111111111101111111111111111111111 | 0.665282649921 |
|    63   | 1111111111111111111111111111111111111111111111111011111111111111 | 0.665314090854 |
|    63   | 0111111111111111111111111111111111111111111111111111111111111111 | 0.666296496847 |
+---------+------------------------------------------------------------------+----------------+


Yes, when removed, some k-mers actually improve the statistics. namely:


|    64   |       | 0.664012942656 |
|    63   |  GCA  | 0.664085601531 |
|    63   |  ACC  | 0.66421763898  |
|    63   |  GTG  | 0.664309399079 |
|    63   |  TCA  | 0.664377058166 |
|    63   |  CAA  | 0.664754734457 |
|    63   |  CGG  | 0.664759742518 |
|    63   |  CTT  | 0.664777537498 |
|    63   |  TCT  | 0.664806259125 |
|    63   |  GGC  | 0.665282649921 |
|    63   |  TAC  | 0.665314090854 |
|    63   |  AAA  | 0.666296496847 |
+---------+-------+----------------+



Does this hold on other training sets? 
We systematically compare, for 20 training sets, the performances with and without the "AAA" 3-mer. In all but 3 / 20 instances; removing this k-mer improves (slighly) the performances: 

	In [108]: by_training_set
	Out[108]: 
	kmer_removed               AAA       GGC       TAC       TCT      none
	training_set                                                          
	-8872022648978232727  0.662299  0.661842  0.657873  0.660434  0.660359
	-7249074755688240261  0.669620  0.667075  0.666971  0.666825  0.668188
	-7145267073968491992  0.678000  0.670598  0.669628  0.668965  0.672905
	-6592428830466920757  0.664882  0.661900  0.658601  0.658892  0.663707
	-6143273851514513120  0.670141  0.665969  0.660908  0.664891  0.666432
	-3719537212837791448  0.667433  0.663843  0.659293  0.664498  0.663001
	-1938437112857461052  0.672234  0.666002  0.667038  0.665832  0.663424
	-1137637855846233358  0.670637  0.671259  0.669931  0.671876  0.673016
	-485722250919234656   0.665312  0.665137  0.661874  0.663355  0.666374
	 129483511320751269   0.669544  0.664127  0.660016  0.664983  0.663586
	 1031413136403321976  0.674658  0.665839  0.668050  0.667481  0.669858
	 1848878592201243769  0.664350  0.662871  0.660595  0.660121  0.660978
	 2445971122342496189  0.664222  0.660987  0.660354  0.664498  0.663556
	 2943334657085702462  0.670227  0.661899  0.662139  0.659481  0.661768
	 3107925978548984657  0.673060  0.667828  0.665784  0.667916  0.671844
	 3161489798832025832  0.664008  0.661910  0.664047  0.661550  0.663600
	 6791097951102390313  0.663885  0.664784  0.664358  0.662087  0.665757
	 6988864376974145320  0.667877  0.662617  0.659928  0.661171  0.663875
	 8035493785773626327  0.674328  0.665659  0.664409  0.666646  0.667404
	 8205434383683132577  0.667233       NaN  0.658897  0.666554  0.662978




The correlation of these k-mers with the class label is : 





# Hyperopt approach 

We install mongodb and the hyperopt package to perform the space search in the space of the 3-4-5 mers randomly subset to only keep at most 64 k-mers. 

Don't know how to perform feature selection with hyperopt.



#Â Cluster analysis

The PCA plot of the 3-mers show a filled V shape; with contigs from the four reigns falling everywhere. The loadings of the first two components show that they are mostly based on GC vs AT content 
We plot a scatter by adding all k-mers purely with GC vs all-kmers purely with AT: no signal

A quick analysis (analyze_PCA.R) with ggplot shows that the first 2 components are GC content and sequence length.

If we normalize for sequence length  we get a flat shaped plot (more expected from a PCA)

We normalize the counts by GC of the originating sequence and do a PCA on it 












