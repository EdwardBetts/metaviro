import os
from snakemake.utils import R

"""TODO:
* Generate short / long reads
* Account for multiple batches 
"""

ALLDOMAIN=["viruses","bact","euk","archea"]


ROOTDIR=os.getcwd()


### BLAST Db generation 
DB_DIR="blast_dbs/"
DATA_DIR="../../data/Genebank/"

## Cat fastas 
ruleorder: cat_euk > cat_fastas

rule cat_euk:
	output: DATA_DIR+"euk.fa"
	# output: "/WINDOWS/euk.fa"
	shell:
		"ls {DATA_DIR}plants/*.fas* {DATA_DIR}fungi/*.fas* | xargs cat > {output}"

rule cat_fastas:
	output:DATA_DIR+"{domain}.fa"
	shell:
		"ls {DATA_DIR}{wildcards.domain}/*.fas* | xargs cat > {output}"

### Index them 

rule make_db:
	# input: "../../data/full_ncbi/{domain}.fa"
	input: DATA_DIR+"{domain}.fa"
	params:
		db_name="{domain}_blastdb",
		db_path="blast_dbs/{domain}_blastdb"
	# output: "{domain}_blastdb.nhr", "{domain}_blastdb.nin", "{domain}_blastdb.nnd", "{domain}_blastdb.nni", "{domain}_blastdb.nog", "{domain}_blastdb.nsd", "{domain}_blastdb.nsi", "{domain}_blastdb.nsq"
	output: DB_DIR+"{domain}_blastdb.ok" # hack to circumvent blastdb inconsistent naming scheme 
	log: "log_make_db.txt"
	shell:
		"makeblastdb -parse_seqids -out {params.db_path} -in {input} -dbtype nucl -max_file_sz 1GB | tee {log} && touch {output}"


rule clean_db:
	shell:
		"rm *_blastdb.n*"

rule list_db_content:
	# input: DB_DIR+"{domain}_blastdb.00.nhr"
	# output: DB_DIR+"{domain}_blastdb_content.txt"
	input: DB_DIR+"{db_name}.ok"
	output: DB_DIR+"{db_name}_content.txt"
	run:
		db_name=os.path.splitext(input[0])[0]
		db_name=DB_DIR+wildcards.db_name
		# print("Considering DB:%s OR %s"%(db_name,wildcards.db_name))
		shell("""blastdbcmd -entry all -db {db_name} -outfmt "%g\t%a\t%i\t%t\t%l" > {output}""")




### Contig sampling 

ruleorder: contigs_sample_indices_short>contigs_sample_indices


rule contigs_sample_indices_short:
	input: DB_DIR+"{domain}_blastdb_content.txt"
	output: "150nt_{ResampleID}/{domain}_sampling_contig_indices.txt"
	params:
		N_CONTIGS_PER_DOMAIN="10000"
	shell:
		"""../../bin/contig_sampling_with_blast.py -l 150 -d 0 -u -n {params.N_CONTIGS_PER_DOMAIN} {input} -o {output}"""

rule contigs_sample_indices:
	input: DB_DIR+"{domain}_blastdb_content.txt"
	output: "{ContigLength}nt_{ResampleID}/{domain}_sampling_contig_indices.txt"
	params:
		N_CONTIGS_PER_DOMAIN="10000"
	shell:
		"""../../bin/contig_sampling_with_blast.py -l {wildcards.ContigLength} -u -n {params.N_CONTIGS_PER_DOMAIN} {input} -o {output}"""

rule contigs_sample_extract_sequences:
	input: seq="{ResampleID}/{domain}_sampling_contig_indices.txt"
	output: contigs="{ResampleID}/{domain}_contigs.fa",sorted_seq=temp("{ResampleID}/{domain}_sampling_contig_indices.sorted.txt")
	params:
		db_path=DB_DIR+"{domain}_blastdb"
	shell:
		"""
		sort -n {input.seq} > {output.sorted_seq}
		blastdbcmd -db {params.db_path} -entry_batch {output.sorted_seq} > {output.contigs}
		"""




rule kmerize:
	input: "{ResampleID}/{domain}_contigs.fa"
	output: "{ResampleID}/{domain}_k{kmerL,\d+}_mers.tsv"
	log:"{ResampleID}/kmerize_{domain}_log.txt"
	shell:
		"../../bin/kmerize.py -k {wildcards.kmerL} {input} -o {output} 2> {log}"


rule measure_kDN:
	input: kmer_file="{ResampleID}/{domain}_k3_mers.tsv",script="sub_group_analysis.R",bact_annot="gi_mapping/bact_annotations_gi.RData",virus_annot="gi_mapping/viruses_annotations_gi.RData"
	output: "{ResampleID}/{domain}_{classLabel}_by_gi_kDN.csv", "{ResampleID}/{domain}_{classLabel}_distribution_kDN.pdf", "{ResampleID}/{domain}_{classLabel}_mean_kDN.csv", "{ResampleID}/{domain}_{classLabel}_nulls_kDN.pdf"
	threads:2
	params:
		N_PERMUTATIONS="10",
		N_GROUP_SAMPLING="10",
		N_NEAREST_NEIGHBORS="73",
		n_majority_class_choice_group="4",
		n_majority_class_choice_sub_group="6",
		n_majority_class_choice_host="6"

	run:
		os.chdir(ROOTDIR)

		if wildcards.classLabel=="Group":
			n_majority_class_choice=params.n_majority_class_choice_group
		elif wildcards.classLabel=="SubGroup":
			n_majority_class_choice=params.n_majority_class_choice_sub_group
		elif wildcards.classLabel=="Host":
			n_majority_class_choice=params.n_majority_class_choice_host

		R("""

			setwd("{wildcards.ResampleID}")
			SOURCE_KMERS="{wildcards.domain}_k3_mers.tsv"
			DOMAIN="{wildcards.domain}"
			GI_MAPPING="../gi_mapping/{wildcards.domain}_annotations_gi.RData"
			classLabelsTag="{wildcards.classLabel}" 
			
			n_cores={threads}
			N_PERMUTATIONS={params.N_PERMUTATIONS}
			N_GROUP_SAMPLING={params.N_GROUP_SAMPLING}
			N_NEAREST_NEIGHBORS={params.N_NEAREST_NEIGHBORS}

			N_MAJORITY_CLASS_LABELS={n_majority_class_choice}

			library(logging,quietly=T)
			logReset()
			addHandler(writeToConsole)
			loginfo("Starting; in dir %s",getwd())
			loginfo("Will process {input.kmer_file}")
			source("../prepare_data.R")
			source("../fast_sparse_knn.R")
			loginfo("Data loaded and annotated")
			source("../sub_group_analysis.R")			
		""")


rule measure_kDN_reign:
	input: "{ResampleID}/archaea_k3_mers.tsv","{ResampleID}/bact_k3_mers.tsv","{ResampleID}/euk_k3_mers.tsv","{ResampleID}/viruses_k3_mers.tsv",script="reign_analysis.R"
	output: "{ResampleID}/all_domains_domain_by_gi_kDN.csv", "{ResampleID}/all_domains_domain_distribution_kDN.pdf", "{ResampleID}/all_domains_domain_mean_kDN.csv", "{ResampleID}/all_domains_domain_nulls_kDN.pdf"
	threads:4
	
	run:
		os.chdir(ROOTDIR)
		R("""
			setwd("{wildcards.ResampleID}")
			classLabelsTag="domain"
			n_cores=4
			N_PERMUTATIONS=10
			N_GROUP_SAMPLING=1
			N_NEAREST_NEIGHBORS=73
			DOMAIN="all_domains"
			N_MAJORITY_CLASS_LABELS=4

			library(logging,quietly=T)
			logReset()
			addHandler(writeToConsole)
			classLabelsTag="domain"
			loginfo("Starting; in dir %s",getwd())

			source("../fast_sparse_knn.R")
			source("../prepare_data_reign.R")
			loginfo("Data loaded and annotated")
			source("../reign_analysis.R")
		""")

ruleorder: measure_kDN_reign > measure_kDN

### GI mapping 

"""We need to convert GenBank GI identifier to the corresponding TaxID in order to get species annotations
We build here the mappings that we'll use
This might take up to 30min and require 4Gb of memory
We follow advices from https://www.biostars.org/p/10959/: 
	If you have a LARGE number of IDS and don't want to be limited by the EUtils pipe there is a set of files in the ftp directory: ftp://ftp.ncbi.nih.gov/pub/taxonomy/ that map TaxIDs to various other identifiers. This would only be helpful if you had too many IDS to submit to EUtils.
We process the taxonomy file, map it back and join it with the GENOME_REPORTS. 
"""

ruleorder: get_precomputed_taxid_maps > map_genome_reports > get_taxid_map

rule get_taxid_map:
	input: 
	output: all_gi=protected("gi_mapping/gi_taxid_nucl.RData"), temp_file=temp("gi_mapping/gi_taxid_nucl.dmp")
	run:
		shell("curl ftp://ftp.ncbi.nih.gov/pub/taxonomy/gi_taxid_nucl.dmp.gz")
		shell("gunzip gi_taxid_nucl.dmp.gz")
		R("""source("prepare_GI_annotations.R")""")

rule map_genome_reports:
	input:"../../data/GENOME_REPORTS/viruses.txt","gi_mapping/gi_taxid_nucl.RData","../../data/GENOME_REPORTS/prokaryotes.txt"
	output:		viruses_gi=protected("gi_mapping/viruses_annotations_gi.RData"),bact_gi=protected("gi_mapping/bact_annotations_gi.RData")
	run:
		R("""source("prepare_GenBank_GenomeReports.R")""")


# rule to copy back precomputed files; to avoid costly re-computation when we force-all new targets 
rule get_precomputed_taxid_maps:
	output: viruses_gi="gi_mapping/viruses_annotations_gi.RData",bact_gi="gi_mapping/bact_annotations_gi.RData"
	input: "gi_mapping_BACK"
	shell:
		"""
		cp  gi_mapping_BACK/* gi_mapping/
		touch gi_mapping/*

		"""


## compress the results 
rule compress_results:
	output:"150nt_all_resample.tgz","500nt_all_resample.tgz","1000nt_all_resample.tgz"
	shell:
		"""
			tar -czvf "150nt_all_resample.tgz" 150nt_Resample*/*.csv
			tar -czvf "500nt_all_resample.tgz" 500nt_Resample*/*.csv
			tar -czvf "1000nt_all_resample.tgz" 1000nt_Resample*/*.csv
		"""
## Some results rules 

rule archea_euk_contigs: 
	input: expand("{length}nt_Resample_{res_id}/{domain}_k3_mers.tsv",length=[150,500,1000],res_id=range(1,100),domain=["euk","archaea"])

rule bact_hardness:
	input: "500nt_Resample_1/bact_Group_distribution_kDN.pdf","500nt_Resample_1/bact_SubGroup_distribution_kDN.pdf","150nt_Resample_2/bact_SubGroup_distribution_kDN.pdf","150nt_Resample_2/bact_Group_distribution_kDN.pdf"

rule test_virus_hardness:
	input: "500nt_Resample_1/viruses_Group_distribution_kDN.pdf","500nt_Resample_1/viruses_SubGroup_distribution_kDN.pdf","150nt_Resample_2/viruses_SubGroup_distribution_kDN.pdf","150nt_Resample_2/viruses_Group_distribution_kDN.pdf"

rule test_all_hardness:
	input: expand("{length}nt_Resample_{res_id}/all_domains_domain_distribution_kDN.pdf",res_id=range(3,10),length=[150,500,1000])


rule all_blast_db:
	input: expand('{domain}_blastdb',domain=ALLDOMAIN)

rule viruses_db:
	input: "blast_dbs/viruses_blastdb.nhr"


rule list_all_contents:
	input: expand(DB_DIR+"{domain}_blastdb_content.txt",domain=ALLDOMAIN)


rule some_hardness:
	input: viruses=expand("{cont_length}nt_Resample_{res_id}/viruses_{classLabels}_distribution_kDN.pdf",res_id=range(1,100),cont_length=[150,500,1000],classLabels=['Group','SubGroup']), bact=expand("{cont_length}nt_Resample_{res_id}/bact_{classLabels}_distribution_kDN.pdf",res_id=range(1,100),cont_length=[150,500,1000],classLabels=['Group','SubGroup'])
